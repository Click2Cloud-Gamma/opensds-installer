# Copyright 2018 The OpenSDS Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---

- name: check ceph exists
  shell: which ceph
  args:
    executable: /bin/bash
  ignore_errors: true
  register: ceph_existence
#
# check installed ceph major version and set it to 'installed_ceph_version'.
#

- name: set_fact installed_ceph_version
  set_fact:
    installed_ceph_version: "{{ result['stdout']  }}"
  when:
    - ceph_existence["rc"] == 0
#
# check 'ceph health' and set the result into 'ceph_health'.
#
- name: check if the installed ceph is healthy or not
  shell: |
    ceph health
  args:
    executable: /bin/bash
  register: result
  ignore_errors: true
  when: ceph_existence["rc"] == 0

- name: set_fact ceph_health
  set_fact:
    # workaround
    ceph_health: "{{ result['stdout'] }}{{ result['stderr'] }}"
  when: ceph_existence["rc"] == 0
#
# check if /etc/ceph/ceph.conf exists or not
#
- name: stat /etc/ceph/ceph.conf
  stat:
    path: /etc/ceph/ceph.conf
  register: ceph_conf_stat

- name: paranoia check for "/etc/ceph/ceph.conf" and its consistency
  fail:
    msg: "/etc/ceph/ceph.conf exists, but ceph cluster status is not HEALTH_OK. Check your environment."
  when:
    - ceph_existence["rc"] == 0
    - ceph_health != "HEALTH_OK"
    - ceph_conf_stat.stat.exists
#
# check ceph cluster member status. (if all the osds are up.)
#
#
# check installed ceph version
#   rc: 0 : required version
#   rc: 1 : older version, upgrade installation requried.
#   rc: 2 : newer version
#
- name: install notario package with pip when ceph backend enabled
  pip:
    name: "{{ item }}"
  with_items:
    - notario
#
# configure opensds.conf ceph section
#
- name: configure opensds global info osdsdock ceph
  ini_file:
    path: "{{ opensds_conf_file }}"
    section: ceph
    option: "{{ item.option }}"
    value: "{{ item.value }}"
  with_items:
        - { option: name, value: "{{ ceph_name }}" }
        - { option: description, value: "{{ ceph_description }}" }
        - { option: driver_name, value: "{{ ceph_driver_name }}" }
        - { option: config_path, value: "{{ ceph_config_path }}" }
  become: yes
  ignore_errors: yes

- name: copy opensds ceph backend file to ceph config file if specify ceph backend
  copy:
    src: ../../../group_vars/ceph/ceph.yaml
    dest: "{{ ceph_config_path }}"

#
# criteria of ceph installation
#
# 1. ceph not installed
# 1-a. 'which ceph' returns 1 #  (most typical case)
#  -> do install
#
# 2. ceph installed, but not configured
# 2-a HEALTH_OK, and cluster member status is NOT healthy
#  -> abort
#  (The pair case is 3-a.)
# 2-b. not HEALTH_OK
#  -> do version check,
#     2-b-1 if ceph ver. is required, do install (and configure).
#     2-b-2 if ceph ver. is older than required, re-install.
#     2-b-3 if ceph ver. is new than required, re-install.

# 3. ceph installed, and configured properly
# 3-a. HEALTH_OK, and cluster member status is healthy
#  -> do version check,
#     3-a-1 if version OK, just configure opensds ceph backend
#     3-a-2 if version NG, abort

# case: 3-a-2

# case: 2-a

# case: 2-b-2, 2-b-3
#
# Uninstall older/newer version ceph components before calling ceph-ansible.
#

# case: 1-a, 2-b-1
#
# Call ceph-ansible playbook.
#
- name: install ceph
  include_tasks: roles/osdsdock/scenarios/ceph_install.yml

#
# Wait until ceph -s returns HEALTH_OK and all the OSDs get up.
#

#
# Create specified ceph pools.
# In case integration with an existing ceph cluster, pool parameters might
# need to be modified. e.g. pg_num So, 'ignore_erros:' is false (default).
- name: create specified pools and initialize them with default pool size.
  shell: ceph osd pool create {{ item }} 100 && ceph osd pool set {{ item }} size 1
#  ignore_errors: yes
  changed_when: false
  with_items: "{{ ceph_pools }}"
#
# In case of a fresh installation, ceph is a bit eventual until pools become
# visible. So, wait.
#
